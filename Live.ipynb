{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press 'Enter' to stop.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = load_model('best_model_CNN.keras')\n",
    "\n",
    "# Settings for audio recording\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "SAMPLE_DURATION = 2  # Process audio every 2 seconds\n",
    "RECORD_SECONDS = SAMPLE_DURATION\n",
    "\n",
    "classes = ['A Better You Podcast', 'Jared', 'Solo Flight Podcast', 'tiana']\n",
    "\n",
    "# Flag to control the recording loop\n",
    "recording = True\n",
    "\n",
    "inputDevice = 9\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK,\n",
    "                    input_device_index=inputDevice)\n",
    "\n",
    "print(\"Recording... Press 'Enter' to stop.\")\n",
    "frames = []\n",
    "\n",
    "# Start recording\n",
    "start_time = time.time()  # To track recording time\n",
    "\n",
    "def process_audio_to_spectrogram(audio_frames, rate):\n",
    "    \"\"\"Converts audio frames to a grey-scale spectrogram.\"\"\"\n",
    "    # Convert audio frames to numpy array\n",
    "    audio_data = np.frombuffer(b''.join(audio_frames), dtype=np.int16)\n",
    "    \n",
    "    # Generate a spectrogram\n",
    "    frequencies, times, spectrogram = signal.spectrogram(audio_data, rate)\n",
    "    \n",
    "    # Convert spectrogram to log scale (dB)\n",
    "    log_spectrogram = np.log(spectrogram + 1e-10)\n",
    "    \n",
    "    # Normalize the spectrogram\n",
    "    log_spectrogram = (log_spectrogram - log_spectrogram.min()) / (log_spectrogram.max() - log_spectrogram.min())\n",
    "    \n",
    "    # Resize the spectrogram to the model's input shape (assuming 128x128x1 as an example)\n",
    "    spectrogram_resized = np.resize(log_spectrogram, (308, 51, 1))\n",
    "    \n",
    "    return spectrogram_resized\n",
    "\n",
    "def predict_speaker(spectrogram):\n",
    "    \"\"\"Predicts the speaker from the spectrogram.\"\"\"\n",
    "    # Reshape for model input and make a prediction\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=0)  # Adding batch dimension\n",
    "    prediction = model.predict(spectrogram, verbose=False)\n",
    "    \n",
    "    index = np.argmax(prediction[0])\n",
    "    \n",
    "    if np.isnan(prediction[0][0]):\n",
    "        return \"Unknown                                                                 \"\n",
    "    \n",
    "    # Return the predicted speaker index\n",
    "    return f'{classes[index]}: {prediction[0][index]*100:.2f}%                                                                       '\n",
    "\n",
    "try:\n",
    "    while recording:\n",
    "        try:\n",
    "            # Read audio data from the stream\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "\n",
    "            # Display elapsed time in seconds\n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "\n",
    "            # Every 2 seconds, process the last 2 seconds of audio\n",
    "            if elapsed_time % RECORD_SECONDS == 0 and elapsed_time > 0:\n",
    "                # Extract the last 2 seconds of frames\n",
    "                num_frames = int(RATE * RECORD_SECONDS / CHUNK)\n",
    "                recent_frames = frames[-num_frames:]\n",
    "                \n",
    "                # Convert to spectrogram and make prediction\n",
    "                spectrogram = process_audio_to_spectrogram(recent_frames, RATE)\n",
    "                speaker_id = predict_speaker(spectrogram)\n",
    "                \n",
    "                print(f\"Predicted Speaker: {speaker_id}\", end=\"\\r\")\n",
    "\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading audio stream: {e}\")\n",
    "            continue\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nRecording stopped by keyboard interrupt.\")\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press 'Enter' to stop.\n",
      "Predicted Speaker: tiana: 100.00%\n",
      "Recording stopped by keyboard interrupt.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import librosa\n",
    "import librosa.display\n",
    "import io\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = load_model('best_model_CNN.keras')\n",
    "\n",
    "# Settings for audio recording\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "SAMPLE_DURATION = 2  # Process audio every 2 seconds\n",
    "RECORD_SECONDS = SAMPLE_DURATION\n",
    "\n",
    "classes = ['A Better You Podcast', 'Jared', 'Solo Flight Podcast', 'tiana']\n",
    "\n",
    "# Flag to control the recording loop\n",
    "recording = True\n",
    "\n",
    "inputDevice = 0\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK,\n",
    "                    input_device_index=inputDevice)\n",
    "\n",
    "print(\"Recording... Press 'Enter' to stop.\")\n",
    "frames = []\n",
    "\n",
    "# Start recording\n",
    "start_time = time.time()  # To track recording time\n",
    "\n",
    "\n",
    "def process_audio_to_spectrogram(frames, sr):\n",
    "    # Convert audio frames to NumPy array\n",
    "    audio_data = np.frombuffer(b''.join(frames), dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "    # Compute the spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "    \n",
    "    # Create a square figure with no axes or padding\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)  # Ensure the figure is square\n",
    "    \n",
    "    # Adjust the aspect ratio to be square\n",
    "    ax.set_aspect('equal')  # This ensures square pixels\n",
    "\n",
    "    # Display the spectrogram\n",
    "    librosa.display.specshow(D, sr=sr, cmap='gray', ax=ax)\n",
    "\n",
    "    # Save figure to a NumPy array\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert to a PIL image and then to a NumPy array\n",
    "    img = Image.open(buf)\n",
    "    spectrogram_array = np.array(img)\n",
    "    \n",
    "    # Convert the image to grayscale using OpenCV\n",
    "    spectrogram_array = cv2.cvtColor(spectrogram_array, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    \n",
    "    # Close the figure\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Normalize the spectrogram\n",
    "    spectrogram_array = spectrogram_array / 255.0\n",
    "\n",
    "    # Resize to match the model's expected input if needed (example size 128x128)\n",
    "\n",
    "    return spectrogram_array\n",
    "\n",
    "\n",
    "def predict_speaker(spectrogram):\n",
    "    \"\"\"Predicts the speaker from the spectrogram.\"\"\"\n",
    "    # Reshape for model input and make a prediction\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=[0, -1])  # Adding batch and channel dimensions\n",
    "    prediction = model.predict(spectrogram, verbose=False)\n",
    "\n",
    "    index = np.argmax(prediction[0])\n",
    "\n",
    "    if np.isnan(prediction[0][0]):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Return the predicted speaker index\n",
    "    return f'{classes[index]}: {prediction[0][index]*100:.2f}%'\n",
    "\n",
    "\n",
    "try:\n",
    "    while recording:\n",
    "        try:\n",
    "            # Read audio data from the stream\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "\n",
    "            # Display elapsed time in seconds\n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "\n",
    "            # Every 2 seconds, process the last 2 seconds of audio\n",
    "            if elapsed_time % RECORD_SECONDS == 0 and elapsed_time > 0:\n",
    "                # Extract the last 2 seconds of frames\n",
    "                num_frames = int(RATE * RECORD_SECONDS / CHUNK)\n",
    "                recent_frames = frames[-num_frames:]\n",
    "\n",
    "                # Convert to spectrogram and make prediction\n",
    "                spectrogram = process_audio_to_spectrogram(recent_frames, RATE)\n",
    "                speaker_id = predict_speaker(spectrogram)\n",
    "\n",
    "                print(f\"Predicted Speaker: {speaker_id}\", end=\"\\r\")\n",
    "\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading audio stream: {e}\")\n",
    "            continue\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nRecording stopped by keyboard interrupt.\")\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press 'Enter' to stop.\n",
      "Predicted Speaker: Solo Flight Podcast: 100.00%                                                                       \n",
      "Recording stopped by keyboard interrupt.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFICAYAAAAyFGczAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEIklEQVR4nO3UsRHAIBDAsJD9d34G4FxDIU3gymtm5gPg8N8OAHiVQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgGCQAMEgAYJBAgSDBAgGCRAMEiAYJEAwSIBgkADBIAGCQQIEgwQIBgkQDBIgGCRAMEiAYJAAwSABgkECBIMECAYJEAwSIBgkQDBIgLAB0vAGjJFb1k4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = load_model('best_model_CNN.keras')\n",
    "\n",
    "# Settings for audio recording\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "SAMPLE_DURATION = 2  # Process audio every 2 seconds\n",
    "RECORD_SECONDS = SAMPLE_DURATION\n",
    "\n",
    "classes = ['3Blue1Brown',\n",
    " 'A Better You Podcast',\n",
    " 'Just Alex Podcast',\n",
    " 'Solo Flight Podcast']\n",
    "\n",
    "# Flag to control the recording loop\n",
    "recording = True\n",
    "\n",
    "inputDevice = 0\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK,\n",
    "                    input_device_index=inputDevice)\n",
    "\n",
    "print(\"Recording... Press 'Enter' to stop.\")\n",
    "frames = []\n",
    "\n",
    "# Start recording\n",
    "start_time = time.time()  # To track recording time\n",
    "\n",
    "def process_audio_to_spectrogram(byte_io):\n",
    "    # Load the audio data from BytesIO\n",
    "    y, sr = librosa.load(byte_io, sr=None)\n",
    "\n",
    "    # Compute the spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "\n",
    "    # Create a square figure with no axes or padding\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)  # Ensure the figure is square\n",
    "    ax.set_axis_off()  # Turn off axis\n",
    "    ax.set_aspect('equal')  # This ensures square pixels\n",
    "\n",
    "    # Display the spectrogram\n",
    "    librosa.display.specshow(D, sr=sr, cmap='gray', ax=ax)\n",
    "\n",
    "    # Save the figure to a BytesIO object\n",
    "    byte_io = io.BytesIO()\n",
    "    plt.savefig(byte_io, bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Seek to the start of the BytesIO object\n",
    "    byte_io.seek(0)\n",
    "\n",
    "    # Convert the BytesIO to a NumPy array (image)\n",
    "    img_array = np.frombuffer(byte_io.getvalue(), np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    return img  # Return the spectrogram for further processing or prediction\n",
    "\n",
    "def saveAudio(audio_frames):\n",
    "    byte_io = io.BytesIO()\n",
    "\n",
    "    # Open the BytesIO object as if it's a file\n",
    "    with wave.open(byte_io, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "    \n",
    "    byte_io.seek(0)  # Seek to the start of the BytesIO object\n",
    "    return byte_io\n",
    "\n",
    "def predict_speaker(spectrogram):\n",
    "    \"\"\"Predicts the speaker from the spectrogram.\"\"\"\n",
    "    # Reshape for model input and make a prediction\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=0)  # Adding batch dimension\n",
    "    prediction = model.predict(spectrogram, verbose=False)\n",
    "    \n",
    "    index = np.argmax(prediction[0])\n",
    "    \n",
    "    if np.isnan(prediction[0][0]):\n",
    "        return \"Unknown                                                                 \"\n",
    "    \n",
    "    # Return the predicted speaker index\n",
    "    return f'{classes[index]}: {prediction[0][index]*100:.2f}%                                                                       '\n",
    "\n",
    "try:\n",
    "    while recording:\n",
    "        try:\n",
    "            # Read audio data from the stream\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "\n",
    "            # Display elapsed time in seconds\n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "\n",
    "            # Every 2 seconds, process the last 2 seconds of audio\n",
    "            if elapsed_time % RECORD_SECONDS == 0 and elapsed_time > 0:\n",
    "                # Extract the last 2 seconds of frames\n",
    "                num_frames = int(RATE * RECORD_SECONDS / CHUNK)\n",
    "                recent_frames = frames[-num_frames:]\n",
    "                \n",
    "                # Convert to spectrogram and make prediction\n",
    "                bytesFilePath = saveAudio(recent_frames)\n",
    "                spectrogram = process_audio_to_spectrogram(bytesFilePath)\n",
    "                speaker_id = predict_speaker(spectrogram)\n",
    "                \n",
    "                print(f\"Predicted Speaker: {speaker_id}\", end=\"\\r\")\n",
    "\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading audio stream: {e}\")\n",
    "            continue\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nRecording stopped by keyboard interrupt.\")\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "0: Microsoft Sound Mapper - Input - Input\n",
      "1: Voicemeeter Out B3 (VB-Audio Vo - Input\n",
      "2: Voicemeeter Out A5 (VB-Audio Vo - Input\n",
      "3: Voicemeeter Out A1 (VB-Audio Vo - Input\n",
      "4: Microphone (Iriun Webcam) - Input\n",
      "5: Voicemeeter Out B1 (VB-Audio Vo - Input\n",
      "6: Voicemeeter Out A2 (VB-Audio Vo - Input\n",
      "7: Voicemeeter Out A4 (VB-Audio Vo - Input\n",
      "8: Voicemeeter Out A3 (VB-Audio Vo - Input\n",
      "9: Microphone (NVIDIA Broadcast) - Input\n",
      "10: Voicemeeter Out B2 (VB-Audio Vo - Input\n",
      "11: Microphone (Blue Snowball) - Input\n",
      "27: Primary Sound Capture Driver - Input\n",
      "28: Voicemeeter Out B3 (VB-Audio Voicemeeter VAIO) - Input\n",
      "29: Voicemeeter Out A5 (VB-Audio Voicemeeter VAIO) - Input\n",
      "30: Voicemeeter Out A1 (VB-Audio Voicemeeter VAIO) - Input\n",
      "31: Microphone (Iriun Webcam) - Input\n",
      "32: Voicemeeter Out B1 (VB-Audio Voicemeeter VAIO) - Input\n",
      "33: Voicemeeter Out A2 (VB-Audio Voicemeeter VAIO) - Input\n",
      "34: Voicemeeter Out A4 (VB-Audio Voicemeeter VAIO) - Input\n",
      "35: Voicemeeter Out A3 (VB-Audio Voicemeeter VAIO) - Input\n",
      "36: Microphone (NVIDIA Broadcast) - Input\n",
      "37: Voicemeeter Out B2 (VB-Audio Voicemeeter VAIO) - Input\n",
      "38: Microphone (Blue Snowball) - Input\n",
      "100: Voicemeeter Out 1 (Voicemeeter Point 1) - Input\n",
      "101: Voicemeeter Out 2 (Voicemeeter Point 2) - Input\n",
      "102: Voicemeeter Out 3 (Voicemeeter Point 3) - Input\n",
      "103: Voicemeeter Out 4 (Voicemeeter Point 4) - Input\n",
      "104: Voicemeeter Out 5 (Voicemeeter Point 5) - Input\n",
      "105: Voicemeeter Out 6 (Voicemeeter Point 6) - Input\n",
      "107: Voicemeeter Out 8 (Voicemeeter Point 8) - Input\n",
      "Recording live audio... Press Ctrl+C to stop.\n",
      "3Blue1Brown: 100.00%\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import os\n",
    "import queue\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from tensorflow.keras.models import load_model\n",
    "import io\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = load_model('best_model_CNN_1.5_Overlap.keras')\n",
    "\n",
    "classes = ['3Blue1Brown', 'A Better You Podcast', 'Just Alex Podcast', 'Solo Flight Podcast']\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Set the duration for each chunk (in seconds)\n",
    "CHUNK_DURATION = 2\n",
    "SAMPLE_RATE = 44100  # Sample rate for audio processing\n",
    "\n",
    "def save_spectrogram(y, sr, chunk_number, output_dir):\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "    ax.set_axis_off()\n",
    "    img = librosa.display.specshow(D, sr=sr, cmap='gray', ax=ax)\n",
    "    ax.set_aspect('equal')    \n",
    "    \n",
    "    # Save the figure to a BytesIO object\n",
    "    byte_io = io.BytesIO()\n",
    "    plt.savefig(byte_io, bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Seek to the start of the BytesIO object\n",
    "    byte_io.seek(0)\n",
    "\n",
    "    # Convert the BytesIO to a NumPy array (image)\n",
    "    img_array = np.frombuffer(byte_io.getvalue(), np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    return img  # Return the spectrogram for further processing or prediction\n",
    "\n",
    "def save_audio_chunk(audio_data, chunk_number, output_dir):\n",
    "    mp3_path = os.path.join(output_dir, f'chunk_{chunk_number}.mp3')\n",
    "    \n",
    "    # Save as WAV first, then convert to MP3\n",
    "    wav_path = os.path.join(output_dir, f'chunk_{chunk_number}.wav')\n",
    "    sf.write(wav_path, audio_data, SAMPLE_RATE)\n",
    "\n",
    "    # Convert WAV to MP3 using pydub\n",
    "    audio_segment = AudioSegment.from_wav(wav_path)\n",
    "    audio_segment.export(mp3_path, format='mp3')\n",
    "\n",
    "    # Remove the temporary WAV file\n",
    "    os.remove(wav_path)\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "\n",
    "    audio_data = indata[:, 0]  # Use the first channel\n",
    "    q.put(audio_data)\n",
    "\n",
    "def predict_speaker(spectrogram):\n",
    "    \"\"\"Predicts the speaker from the spectrogram.\"\"\"\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=0)  # Adding batch dimension\n",
    "    prediction = model.predict(spectrogram, verbose=False)\n",
    "    \n",
    "    index = np.argmax(prediction[0])\n",
    "    \n",
    "    if np.isnan(prediction[0][0]):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    return f'{classes[index]}: {prediction[0][index]*100:.2f}%'\n",
    "\n",
    "def process_live_audio(output_dir, chunk_duration=CHUNK_DURATION):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    global q\n",
    "    q = queue.Queue()\n",
    "    chunk_sample_count = SAMPLE_RATE * chunk_duration\n",
    "\n",
    "    list_audio_devices()\n",
    "\n",
    "    device_index = int(input(\"Enter the device index for input: \"))\n",
    "    \n",
    "    all_audio_buffer = np.array([])\n",
    "\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=SAMPLE_RATE, device=device_index):\n",
    "        print(\"Recording live audio... Press Ctrl+C to stop.\")\n",
    "        chunk_number = 1\n",
    "        while True:\n",
    "            try:\n",
    "                audio_chunk = q.get()\n",
    "\n",
    "                # Append the new audio chunk to the buffer\n",
    "                all_audio_buffer = np.concatenate((all_audio_buffer, audio_chunk))\n",
    "\n",
    "                # # Keep only the last 2 seconds of audio\n",
    "                # if len(all_audio_buffer) > chunk_sample_count:\n",
    "                #     all_audio_buffer = all_audio_buffer[-chunk_sample_count:]\n",
    "\n",
    "                # Save the audio if we have enough data\n",
    "                if len(all_audio_buffer) >= chunk_sample_count:\n",
    "                    chunk = all_audio_buffer.copy()  # Get the current buffer\n",
    "                    chunk = chunk[-chunk_sample_count:]\n",
    "\n",
    "                    # Save the spectrogram\n",
    "                    audio_img = save_spectrogram(chunk, SAMPLE_RATE, chunk_number, output_dir)\n",
    "                    \n",
    "                    # Save the audio chunk as MP3\n",
    "                    # save_audio_chunk(chunk, chunk_number, output_dir)\n",
    "                    \n",
    "                    print(predict_speaker(audio_img), end=\"\\r\")\n",
    "                    chunk_number += 1\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Recording stopped.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                raise\n",
    "                print(f\"Error processing audio chunk: {e}\")\n",
    "\n",
    "def list_audio_devices():\n",
    "    devices = sd.query_devices()\n",
    "    print(\"Available audio devices:\")\n",
    "    for i, device in enumerate(devices):\n",
    "        try:\n",
    "            with sd.InputStream(callback=audio_callback, channels=1, samplerate=SAMPLE_RATE, device=i):\n",
    "                audio_chunk = q.get()\n",
    "            print(f\"{i}: {device['name']} - {'Input' if device['max_input_channels'] > 0 else 'Output'}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SPECTROGRAM_FOLDER = \"Dataset/Spectrogram\"\n",
    "    process_live_audio(SPECTROGRAM_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n",
      "Running function on the last 2 seconds of audio...\n",
      "Processed audio saved as last_two_seconds.wav\n",
      "Recording...\n",
      "Recording finished.\n",
      "Running function on the last 2 seconds of audio...\n",
      "Processed audio saved as last_two_seconds.wav\n",
      "Recording...\n",
      "Recording finished.\n",
      "Running function on the last 2 seconds of audio...\n",
      "Processed audio saved as last_two_seconds.wav\n",
      "Recording...\n",
      "Recording finished.\n",
      "Running function on the last 2 seconds of audio...\n",
      "Processed audio saved as last_two_seconds.wav\n",
      "Recording...\n",
      "Recording finished.\n",
      "Running function on the last 2 seconds of audio...\n",
      "Processed audio saved as last_two_seconds.wav\n",
      "Recording...\n",
      "Recording stopped.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import os\n",
    "import queue\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from tensorflow.keras.models import load_model\n",
    "import io\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "# Get available devices\n",
    "# print(\"Available recording devices:\")\n",
    "# print(sd.query_devices())\n",
    "\n",
    "# # Ask the user to select a device\n",
    "# device_id = int(input(\"Select a device ID for recording: \"))\n",
    "device_id = 5\n",
    "\n",
    "# Parameters\n",
    "fs = 44100  # Sample rate\n",
    "\n",
    "def RUN(audio_data, chunk_number, output_dir):\n",
    "    # Example function to process audio data\n",
    "    print(\"Running function on the last 2 seconds of audio...\")\n",
    "    # Saving the audio to a WAV file\n",
    "    with wave.open('last_two_seconds.wav', 'wb') as wf:\n",
    "        wf.setnchannels(2)\n",
    "        wf.setsampwidth(2)  # 2 bytes for int16\n",
    "        wf.setframerate(fs)\n",
    "        wf.writeframes(audio_data.tobytes())\n",
    "    print(\"Processed audio saved as last_two_seconds.wav\")\n",
    "    \n",
    "    \n",
    "    # Save the spectrogram\n",
    "    # audio_img = save_spectrogram(audio_data, SAMPLE_RATE, chunk_number, output_dir)\n",
    "    # print(predict_speaker(audio_img), end=\"\\r\")\n",
    "    \n",
    "    # Save the audio chunk as MP3\n",
    "    # save_audio_chunk(audio_data, chunk_number, output_dir)\n",
    "    \n",
    "\n",
    "def process_wav_file(file_path, output_dir, chunk_duration=CHUNK_DURATION, overlap_duration=1.5):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't Load: {file_path} - {e}\")\n",
    "        return\n",
    "\n",
    "    # Skip very short files\n",
    "    if len(y) < sr * chunk_duration:\n",
    "        print(f\"Skipping short file: {file_path} (length: {len(y)/sr:.2f} seconds)\")\n",
    "        return    \n",
    "\n",
    "    img = save_spectrogram(y, sr)\n",
    "    \n",
    "    print(predict_speaker(img))\n",
    "\n",
    "def save_spectrogram(y, sr):\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "    ax.set_axis_off()\n",
    "    img = librosa.display.specshow(D, sr=sr, cmap='gray', ax=ax)\n",
    "    ax.set_aspect('equal')    \n",
    "    \n",
    "    # Save the figure to a BytesIO object\n",
    "    byte_io = io.BytesIO()\n",
    "    plt.savefig(byte_io, bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Seek to the start of the BytesIO object\n",
    "    byte_io.seek(0)\n",
    "\n",
    "    # Convert the BytesIO to a NumPy array (image)\n",
    "    img_array = np.frombuffer(byte_io.getvalue(), np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    return img  # Return the spectrogram for further processing or prediction\n",
    "\n",
    "def predict_speaker(spectrogram):\n",
    "    \"\"\"Predicts the speaker from the spectrogram.\"\"\"\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=0)  # Adding batch dimension\n",
    "    prediction = model.predict(spectrogram, verbose=False)\n",
    "    \n",
    "    index = np.argmax(prediction[0])\n",
    "    \n",
    "    if np.isnan(prediction[0][0]):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    return f'{classes[index]}: {prediction[0][index]*100:.2f}%'\n",
    "\n",
    "SPECTROGRAM_FOLDER = \"Dataset/Spectrogram\"\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "chunk_number = 1\n",
    "try:\n",
    "    while True:\n",
    "        \n",
    "        # Record audio for 2 seconds\n",
    "        print(\"Recording...\")\n",
    "        recording = sd.rec(int(2 * fs), samplerate=fs, channels=2, dtype='int16', device=device_id)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        print(\"Recording finished.\")\n",
    "        \n",
    "        # Call the RUN function with the recorded audio\n",
    "        RUN(recording, chunk_number, SPECTROGRAM_FOLDER)\n",
    "        chunk_number += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Recording stopped.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
